{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import normalize\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import auxiliary_fun as a\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "from keras import utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification as mk\n",
    "from keras.metrics import Precision, Recall, AUC, MeanSquaredError\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "Xor = normalize(digits.data)\n",
    "yor = digits.target\n",
    "print(len(Xor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the dominating dataset with $\\varepsilon \\le 0.2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "index = a.dominatingSet(Xor,yor,epsilon=0.2)\n",
    "Xsub=Xor[index]\n",
    "ysub=yor[index]\n",
    "end = time.time()\n",
    "print(\"Dominating dataset, time for computation: \",end - start)\n",
    "print(\"Dominating set size: \",np.shape(Xsub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a neural network and evaluating on the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_yor = utils.to_categorical(yor,10)\n",
    "cat_ysub = utils.to_categorical(ysub,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(5):\n",
    "    model1 = Sequential()\n",
    "    model1.add(Dense(units=400, activation='sigmoid', input_shape=(64,)))\n",
    "    model1.add(Dense(units=800, activation='sigmoid'))\n",
    "    model1.add(Dense(units=300, activation='sigmoid'))\n",
    "    model1.add(Dense(units=800, activation='sigmoid'))\n",
    "    model1.add(Dense(units=300, activation='sigmoid'))\n",
    "    model1.add(Dense(units=10, activation='softmax'))\n",
    "    model1.compile(optimizer=\"Adam\", \n",
    "                   loss='categorical_crossentropy', \n",
    "                   metrics=['accuracy',\n",
    "                            Recall(), \n",
    "                            Precision(),\n",
    "                            AUC(),\n",
    "                            MeanSquaredError()])\n",
    "    start = time.time()\n",
    "    history1=model1.fit(Xor, cat_yor,batch_size=len(Xor), \n",
    "                                #batch_size=1,\n",
    "                                #validation_split = val_split, \n",
    "                                epochs=epochs,\n",
    "                                verbose=False)\n",
    "    end = time.time()\n",
    "    print(abs(start-end))\n",
    "    l.append(model1.evaluate(Xor,cat_yor,verbose=0))\n",
    "l = np.array(l)\n",
    "print(l)\n",
    "plt.plot(history1.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = []\n",
    "for i in range(5):\n",
    "    model2 = Sequential()\n",
    "    model2.add(Dense(units=400, activation='sigmoid', input_shape=(64,)))\n",
    "    model2.add(Dense(units=800, activation='sigmoid'))\n",
    "    model2.add(Dense(units=300, activation='sigmoid'))\n",
    "    model2.add(Dense(units=800, activation='sigmoid'))\n",
    "    model2.add(Dense(units=300, activation='sigmoid'))\n",
    "    model2.add(Dense(units=10, activation='softmax'))\n",
    "    model2.compile(optimizer=\"Adam\", \n",
    "                   loss='categorical_crossentropy', \n",
    "                   metrics=['accuracy',\n",
    "                            Recall(), \n",
    "                            Precision(),\n",
    "                            AUC(),\n",
    "                            MeanSquaredError()])\n",
    "    import time\n",
    "    start = time.time()\n",
    "    history2=model2.fit(Xsub, cat_ysub,batch_size=len(Xsub),\n",
    "                                #batch_size=1,\n",
    "                                #validation_split = val_split, \n",
    "                                epochs=epochs,\n",
    "                                verbose=False)\n",
    "    end = time.time()\n",
    "    print(abs(start-end))\n",
    "    l2.append(model2.evaluate(Xor,cat_yor,verbose=0))\n",
    "l2=np.array(l2)\n",
    "print(l2)\n",
    "plt.plot(history2.history['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X=np.column_stack((Xor,yor))\n",
    "np.random.shuffle(X)\n",
    "Xor = X[:,0:64]\n",
    "yor = X[:,64]\n",
    "cat_yor = utils.to_categorical(yor,10)\n",
    "random_index = random.sample(range(len(Xor)),len(Xsub))\n",
    "Xrand = Xor[random_index]\n",
    "yrand = yor[random_index]\n",
    "cat_yrand = utils.to_categorical(yrand,10)\n",
    "digits_random=np.column_stack((Xrand,yrand))\n",
    "#np.savetxt(\"digits_random3.txt\",digits_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#digits_random=np.column_stack((Xrand,yrand))\n",
    "#np.savetxt(\"digits_random.txt\",digits_random)\n",
    "digits_random=np.loadtxt(\"data/digits_random.txt\")\n",
    "Xrand = digits_random[:,0:64]\n",
    "yrand = digits_random[:,64]\n",
    "cat_yrand = utils.to_categorical(yrand,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3 = []\n",
    "for i in range(5):\n",
    "    model3 = Sequential()\n",
    "    model3.add(Dense(units=400, activation='sigmoid', input_shape=(64,)))\n",
    "    model3.add(Dense(units=800, activation='sigmoid'))\n",
    "    model3.add(Dense(units=300, activation='sigmoid'))\n",
    "    model3.add(Dense(units=800, activation='sigmoid'))\n",
    "    model3.add(Dense(units=300, activation='sigmoid'))\n",
    "    model3.add(Dense(units=10, activation='softmax'))\n",
    "    model3.compile(optimizer=\"Adam\", \n",
    "                   loss='categorical_crossentropy', \n",
    "                   metrics=['accuracy',\n",
    "                            Recall(), \n",
    "                            Precision(),\n",
    "                            AUC(),\n",
    "                            MeanSquaredError()])\n",
    "    history3=model3.fit(Xrand, cat_yrand, \n",
    "                        batch_size=len(Xrand), \n",
    "                        epochs= epochs, \n",
    "                        verbose=False)\n",
    "    l3.append(model3.evaluate(Xor,cat_yor,verbose=0))\n",
    "l3 = np.array(l3)\n",
    "print(l3)\n",
    "plt.plot(history3.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(l3[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hausdorff and Bottleneck distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import directed_hausdorff\n",
    "print(\"Hausdorff distance between the original dataset and the dominating dataset: \",max(directed_hausdorff(Xor, Xsub)[0], directed_hausdorff(Xsub, Xor)[0])/2)\n",
    "print(\"Hausdorff distance between the original dataset and the random dataset: \",max(directed_hausdorff(Xor, Xrand)[0], directed_hausdorff(Xrand, Xor)[0])/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ripser import ripser\n",
    "diagrams_or = ripser(Xor,maxdim=2)['dgms']\n",
    "diagrams_Sub = ripser(Xsub,maxdim=2)['dgms']\n",
    "diagrams_Rand = ripser(Xrand,maxdim=2)['dgms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gudhi as g\n",
    "message = \"Bottleneck distance for dominating dataset and dimension 0 =\" + '%.2f' % g.bottleneck_distance(diagrams_or[0], diagrams_Sub[0])\n",
    "print(message)\n",
    "message = \"Bottleneck distance for dominating dataset and dimension 1 =\" + '%.2f' % g.bottleneck_distance(diagrams_or[1], diagrams_Sub[1])\n",
    "print(message)\n",
    "message = \"Bottleneck distance for Random dataset and dimension 0 =\" + '%.2f' % g.bottleneck_distance(diagrams_or[0], diagrams_Rand[0])\n",
    "print(message)\n",
    "message = \"Bottleneck distance for Random dataset and dimension 1 =\" + '%.2f' % g.bottleneck_distance(diagrams_or[1], diagrams_Rand[1])\n",
    "print(message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.homology import VietorisRipsPersistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homology_dimensions=[0,1,2]\n",
    "persistence = VietorisRipsPersistence(\n",
    "    metric = \"euclidean\",\n",
    "    homology_dimensions = homology_dimensions,\n",
    ")\n",
    "diagrams_basic = persistence.fit_transform([Xor,Xsub,Xrand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.plotting import plot_diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diagram(diagrams_basic[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
